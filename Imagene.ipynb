{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "diehcNXa2aEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f32ea7-11f4-42dc-8842-83a4108fd65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print True if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python diffusers torch torchvision transformers accelerate numpy pillow opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrtU2XdsN4Y2",
        "outputId": "bb5db7dc-59a6-4a3e-a380-737f36821c72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.0)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=6008051 sha256=94555d7daa404de706be57911e909aff9f202306c9e32e5c23ce07f501fcb67a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, diskcache, nvidia-cusparse-cu12, nvidia-cudnn-cu12, llama-cpp-python, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuxcwI75N9Kv",
        "outputId": "d0e98815-2e92-44e3-fc36-b2d62b8a6fa3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pq1FhH9PJLJ",
        "outputId": "d0402dea-d285-4640-9b2f-d41abe57680f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python torch diffusers transformers accelerate safetensors matplotlib Pillow gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0HCSWZKPQMw",
        "outputId": "b5835cec-876a-4627-87b3-01743638c37e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Load the fine-tuned model\n",
        "def load_fine_tuned_model(model_path=None):\n",
        "    if not model_path:\n",
        "        model_path = '/content/drive/MyDrive/fine_tuned_tiny_stories_model'\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "\n",
        "        print(f\"Model loaded successfully and moved to {device}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading a fallback model if the fine-tuned one fails\n",
        "        try:\n",
        "            print(\"Attempting to load fallback model gpt2...\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            print(f\"Fallback model loaded successfully and moved to {device}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading fallback model: {e2}\")\n",
        "            return None, None\n",
        "\n",
        "# Helper functions for story generation\n",
        "def extract_character_name(prompt):\n",
        "    character_names = [\"Spider-Man\", \"Peter Parker\", \"Batman\", \"Superman\", \"Wonder Woman\",\n",
        "                       \"Iron Man\", \"Captain America\", \"Thor\", \"Hulk\", \"Black Widow\"]\n",
        "\n",
        "    for name in character_names:\n",
        "        if name.lower() in prompt.lower():\n",
        "            return name\n",
        "\n",
        "    words = prompt.split()\n",
        "    for word in words:\n",
        "        if word[0].isupper() and len(word) > 1 and word.lower() not in [\"write\", \"story\", \"about\"]:\n",
        "            return word\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_topic(prompt):\n",
        "    cleaned = prompt.lower().replace(\"write\", \"\").replace(\"story\", \"\").replace(\"about\", \"\")\n",
        "    keywords = [\"adventure\", \"flying\", \"jumping\", \"fighting\", \"learning\", \"playing\", \"making friends\"]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        if keyword in cleaned:\n",
        "            return keyword\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_story(model, tokenizer, prompt, max_length=400, retries=3):\n",
        "    story_prompt = f\"Write a simple children's story about {prompt}. Use simple language and short sentences.\"\n",
        "\n",
        "    # Increase repetition penalty and adjust parameters for better generation\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            input_ids = tokenizer.encode(story_prompt, return_tensors='pt')\n",
        "            device = next(model.parameters()).device\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Use different generation parameters for each retry attempt\n",
        "            temperature = 0.7 + (attempt * 0.1)  # Gradually increase temperature with each retry\n",
        "            repetition_penalty = 1.2 + (attempt * 0.1)  # Gradually increase repetition penalty\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,  # Increased from 2\n",
        "                top_k=50,  # Increased from 40\n",
        "                top_p=0.92,  # Adjusted from 0.9\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=repetition_penalty\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up the generated text to focus on the actual story\n",
        "            if story_prompt in generated_text:\n",
        "                generated_text = generated_text.replace(story_prompt, \"\").strip()\n",
        "\n",
        "            # Check if we have a reasonable story\n",
        "            if len(generated_text) > 100 and \".\" in generated_text:\n",
        "                print(f\"Successfully generated story on attempt {attempt+1}\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1} produced insufficient content ({len(generated_text)} chars). Retrying...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in story generation attempt {attempt+1}: {e}\")\n",
        "\n",
        "    # If all retries fail, generate a more varied story using a different approach\n",
        "    print(\"All generation attempts failed. Using alternative generation method...\")\n",
        "    return generate_alternative_story(prompt)\n",
        "\n",
        "def generate_alternative_story(prompt):\n",
        "    \"\"\"\n",
        "    Generate a more varied story when the model-based generation fails.\n",
        "    This creates a more dynamic story than the previous structured approach.\n",
        "    \"\"\"\n",
        "    character = extract_character_name(prompt) or \"the little hero\"\n",
        "    topic = extract_topic(prompt) or \"having an adventure\"\n",
        "\n",
        "    # Create a list of possible story templates\n",
        "    story_templates = [\n",
        "        f\"\"\"Once upon a time, there was {character} who lived in a small village.\n",
        "{character} loved {topic} more than anything else.\n",
        "\n",
        "One day, {character} discovered something magical while playing outside. It was a tiny glowing ball that could float in the air.\n",
        "\n",
        "\"What's this?\" wondered {character}. The ball started to move, and {character} followed it into the forest.\n",
        "\n",
        "The ball led {character} to a hidden garden where other children were playing with similar magical balls. They welcomed {character} and taught them new games.\n",
        "\n",
        "{character} had so much fun that day! When it was time to go home, one of the children gave {character} a magical ball of their own.\n",
        "\n",
        "From that day on, {character} visited the hidden garden often and made many new friends. They all loved {topic} together and had wonderful adventures.\"\"\",\n",
        "\n",
        "        f\"\"\"{character} woke up early one morning with a big smile. Today was the day for {topic}!\n",
        "\n",
        "{character} packed a small bag with snacks and a water bottle. \"I'm ready for my adventure!\" {character} said.\n",
        "\n",
        "Outside, {character} met a friendly cat. \"Hello, cat! Would you like to join me for {topic}?\" The cat meowed and followed along.\n",
        "\n",
        "As they walked through the park, they found a lost teddy bear. \"We should find who this belongs to,\" said {character}.\n",
        "\n",
        "They asked everyone they met about the teddy bear. Finally, they met a little girl who was crying. \"My teddy!\" she said happily when she saw it.\n",
        "\n",
        "\"Thank you for helping me find my teddy,\" the girl smiled. \"Would you like to play with me?\"\n",
        "\n",
        "{character}, the cat, and the girl spent the whole day playing and having fun with {topic}. It was the best day ever!\"\"\",\n",
        "\n",
        "        f\"\"\"The sun was shining brightly when {character} decided it was the perfect day for {topic}.\n",
        "\n",
        "\"I've never tried {topic} before,\" {character} thought, \"but today I will be brave!\"\n",
        "\n",
        "{character} began slowly, taking small steps. It wasn't easy at first, and {character} almost gave up.\n",
        "\n",
        "\"Don't worry, you can do it!\" said a friendly voice. {character} looked up and saw a smiling butterfly.\n",
        "\n",
        "With the butterfly's encouragement, {character} tried again. This time, {character} did much better!\n",
        "\n",
        "Soon, {character} was having so much fun with {topic}. \"Thank you for believing in me,\" {character} said to the butterfly.\n",
        "\n",
        "The butterfly fluttered around happily. \"Sometimes we just need a friend to help us be brave,\" it replied.\n",
        "\n",
        "{character} went home that evening with a happy heart, excited to try {topic} again tomorrow.\"\"\"\n",
        "    ]\n",
        "\n",
        "    import random\n",
        "    return random.choice(story_templates)\n",
        "\n",
        "def split_story_into_panels(story, num_panels=4):\n",
        "    if len(story) < 100:\n",
        "        print(\"Warning: Story is very short. Panel division may not be optimal.\")\n",
        "\n",
        "    paragraphs = story.split('\\n')\n",
        "    paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "    dialogues = []\n",
        "    panel_texts = []\n",
        "\n",
        "    if len(paragraphs) >= num_panels:\n",
        "        if len(paragraphs) > num_panels:\n",
        "            step = len(paragraphs) / num_panels\n",
        "            panels = []\n",
        "            for i in range(num_panels):\n",
        "                start_idx = int(i * step)\n",
        "                end_idx = int((i + 1) * step) if i < num_panels - 1 else len(paragraphs)\n",
        "                panel_text = \" \".join(paragraphs[start_idx:end_idx])\n",
        "                panels.append(panel_text)\n",
        "\n",
        "                dialogue = extract_dialogue(panel_text)\n",
        "                dialogues.append(dialogue)\n",
        "\n",
        "                clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "        else:\n",
        "            panels = paragraphs[:num_panels]\n",
        "            for panel in panels:\n",
        "                dialogue = extract_dialogue(panel)\n",
        "                dialogues.append(dialogue)\n",
        "                clean_text = panel.replace(dialogue, \"\") if dialogue else panel\n",
        "                panel_texts.append(clean_text)\n",
        "            return panels, panel_texts, dialogues\n",
        "\n",
        "    sentences = []\n",
        "    for chunk in story.split('. '):\n",
        "        sentences.extend([s.strip() + '.' for s in chunk.split('! ')])\n",
        "    sentences = [s for s in sentences if len(s) > 1]\n",
        "\n",
        "    if len(sentences) < num_panels:\n",
        "        while len(sentences) < num_panels:\n",
        "            longest_idx = max(range(len(sentences)), key=lambda i: len(sentences[i]))\n",
        "            longest = sentences[longest_idx]\n",
        "\n",
        "            if ', ' in longest:\n",
        "                parts = longest.split(', ', 1)\n",
        "                sentences[longest_idx] = parts[0] + '.'\n",
        "                sentences.insert(longest_idx + 1, parts[1])\n",
        "            else:\n",
        "                sentences.append(f\"Meanwhile, {longest.lower()}\")\n",
        "\n",
        "    step = max(1, len(sentences) // num_panels)\n",
        "\n",
        "    panels = []\n",
        "    for i in range(num_panels):\n",
        "        start_idx = i * step\n",
        "        end_idx = (i + 1) * step if i < num_panels - 1 else len(sentences)\n",
        "        panel_text = \" \".join(sentences[start_idx:end_idx])\n",
        "        panels.append(panel_text)\n",
        "\n",
        "        dialogue = extract_dialogue(panel_text)\n",
        "        dialogues.append(dialogue)\n",
        "\n",
        "        clean_text = panel_text.replace(dialogue, \"\") if dialogue else panel_text\n",
        "        panel_texts.append(clean_text)\n",
        "\n",
        "    return panels, panel_texts, dialogues\n",
        "\n",
        "def extract_dialogue(text):\n",
        "    import re\n",
        "    dialogue_matches = re.findall(r'\"([^\"]*)\"', text)\n",
        "    if dialogue_matches:\n",
        "        return dialogue_matches[0]\n",
        "    return \"\"\n",
        "\n",
        "def load_custom_model():\n",
        "    print(\"Loading InkoloRA model - this may take a while...\")\n",
        "\n",
        "    custom_model_path = \"/content/drive/MyDrive/InkoloRA.safetensors\"\n",
        "\n",
        "    if not os.path.exists(custom_model_path):\n",
        "        print(f\"Warning: Custom model not found at {custom_model_path}. Falling back to Stable Diffusion XL.\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            \"runwayml/stable-diffusion-v1-5\",\n",
        "            torch_dtype=torch.float16,\n",
        "            safety_checker=None\n",
        "        )\n",
        "\n",
        "        pipe.unet.load_attn_procs(custom_model_path)\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if device == \"cpu\":\n",
        "            print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        print(\"InkoloRA model loaded successfully!\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading custom model: {e}\")\n",
        "        print(\"Falling back to Stable Diffusion XL...\")\n",
        "        return load_sdxl_model()\n",
        "\n",
        "def load_sdxl_model():\n",
        "    print(\"Loading SDXL model - this may take a while...\")\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: CUDA not available, using CPU. This will be very slow.\")\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def add_speech_bubble(image, text, position=None):\n",
        "    if not text:\n",
        "        return image\n",
        "\n",
        "    img = np.array(image.convert(\"RGBA\"))\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    if position is None:\n",
        "        position = (width // 2, height // 4)\n",
        "\n",
        "    bubble = np.zeros((height, width, 4), dtype=np.uint8)\n",
        "\n",
        "    bubble_width = min(300, max(150, len(text) * 10))\n",
        "    bubble_height = min(130, max(60, (len(text) // 20 + 1) * 30))\n",
        "\n",
        "    cv2.ellipse(bubble, position, (bubble_width, bubble_height), 0, 0, 360, (255, 255, 255, 255), -1)\n",
        "\n",
        "    pointer_x = position[0]\n",
        "    pointer_y = position[1] + bubble_height\n",
        "    triangle_pts = np.array([\n",
        "        [pointer_x - 20, pointer_y - 10],\n",
        "        [pointer_x + 20, pointer_y - 10],\n",
        "        [pointer_x, pointer_y + 30]\n",
        "    ])\n",
        "    cv2.fillPoly(bubble, [triangle_pts], (255, 255, 255, 255))\n",
        "\n",
        "    bubble_pil = Image.fromarray(bubble)\n",
        "\n",
        "    draw = ImageDraw.Draw(bubble_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except:\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "    wrapped_text = wrap_text(text, font, bubble_width - 40)\n",
        "\n",
        "    text_x = position[0] - (bubble_width // 2) + 20\n",
        "    text_y = position[1] - (bubble_height // 2) + 10\n",
        "\n",
        "    for i, line in enumerate(wrapped_text):\n",
        "        draw.text((text_x, text_y + i * 30), line, fill=\"black\", font=font)\n",
        "\n",
        "    return Image.alpha_composite(image.convert(\"RGBA\"), bubble_pil)\n",
        "\n",
        "def wrap_text(text, font, max_width):\n",
        "    words = text.split()\n",
        "    wrapped_lines = []\n",
        "    current_line = []\n",
        "\n",
        "    for word in words:\n",
        "        test_line = ' '.join(current_line + [word])\n",
        "        try:\n",
        "            try:\n",
        "                bbox = font.getbbox(test_line)\n",
        "                line_width = bbox[2] - bbox[0]\n",
        "            except AttributeError:\n",
        "                try:\n",
        "                    line_width = font.getlength(test_line)\n",
        "                except AttributeError:\n",
        "                    line_width = font.getsize(test_line)[0]\n",
        "        except:\n",
        "            line_width = len(test_line) * 10\n",
        "\n",
        "        if line_width <= max_width:\n",
        "            current_line.append(word)\n",
        "        else:\n",
        "            wrapped_lines.append(' '.join(current_line))\n",
        "            current_line = [word]\n",
        "\n",
        "    if current_line:\n",
        "        wrapped_lines.append(' '.join(current_line))\n",
        "\n",
        "    return wrapped_lines\n",
        "\n",
        "def get_template_data(template_name):\n",
        "    templates = {\n",
        "        \"batman\": {\n",
        "            \"prompts\": [\n",
        "                \"Batman standing on a dark Gotham rooftop, his cape flowing in the wind, the Bat-Signal glowing in the sky behind him.\",\n",
        "                \"Close-up shot of Batman's determined face, shadows partially covering his mask, eyes glowing in the darkness.\",\n",
        "                \"Batman leaping from a rooftop, his cape spread like wings as he descends towards a group of criminals.\",\n",
        "                \"Batman delivering a powerful punch to a thug, the action captured in a comic-style motion blur.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"Gotham never sleeps, and neither do I.\",\n",
        "                \"Justice isn't a choice. It's a responsibility.\",\n",
        "                \"Fear is a tool. I use it against them.\",\n",
        "                \"Crime has no place in my city.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (300, 50), (220, 120), (280, 140)],\n",
        "            \"story\": \"\"\"The Dark Knight watches over Gotham City from high above. Tonight is like any other night for Batman - a city full of shadows that need his protection.\n",
        "\n",
        "As the Bat-Signal illuminates the cloudy sky, Batman's eyes narrow with determination. The signal isn't just a call for help - it's a reminder of the oath he took to protect the innocent.\n",
        "\n",
        "Batman spots a group of criminals attempting to rob a local shop. He leaps from the rooftop, his cape billowing around him like massive wings. The criminals look up in terror as his silhouette blots out the moonlight.\n",
        "\n",
        "With swift, calculated movements, Batman takes down the criminals one by one. Another night in Gotham, another battle won. But Batman knows his work is never truly done. As long as there is crime in Gotham, he will be there to fight it.\"\"\"\n",
        "        },\n",
        "        \"spiderman\": {\n",
        "            \"prompts\": [\n",
        "                \"Spider-Man perched on the edge of a skyscraper, overlooking New York City, his iconic red and blue suit vibrant against the skyline.\",\n",
        "                \"Spider-Man shooting webs from his wrists, swinging between buildings with great agility.\",\n",
        "                \"Spider-Man facing off against a villain, in a dynamic action pose, ready to fight.\",\n",
        "                \"Spider-Man helping a civilian, showcasing his friendly neighborhood hero nature.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"With great power comes great responsibility.\",\n",
        "                \"Just your friendly neighborhood Spider-Man!\",\n",
        "                \"My spider-sense is tingling!\",\n",
        "                \"I'm not going to let anyone get hurt on my watch.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (280, 80), (240, 110), (260, 90)],\n",
        "            \"story\": \"\"\"Peter Parker sits perched on the edge of a towering skyscraper, overlooking the bustling streets of New York City. His uncle Ben's words echo in his mind as he surveys the city he's sworn to protect.\n",
        "\n",
        "With a flick of his wrist, Spider-Man launches himself off the building, shooting a web that attaches to a nearby structure. He swings gracefully through the concrete canyons of Manhattan, the wind rushing past his mask as pedestrians below point and cheer.\n",
        "\n",
        "Suddenly, his spider-sense alerts him to danger. A bank robbery is in progress just a few blocks away. Spider-Man changes direction mid-swing, heading straight for the trouble. As he arrives, he sees masked robbers threatening innocent civilians.\n",
        "\n",
        "With swift movements and well-placed webs, Spider-Man subdues the criminals and ensures everyone is safe. A young child approaches him with wide eyes of admiration. Spider-Man kneels down to their level, reminding them that anyone can be a hero if they choose to help others.\"\"\"\n",
        "        },\n",
        "        \"wonderwoman\": {\n",
        "            \"prompts\": [\n",
        "                \"Wonder Woman standing heroically, her golden lasso and bracelets gleaming, an island paradise visible in the background.\",\n",
        "                \"Wonder Woman deflecting bullets with her bracelets, her expression determined and fierce.\",\n",
        "                \"Wonder Woman wielding her sword and shield, engaged in an epic battle against mythological creatures.\",\n",
        "                \"Wonder Woman using her lasso of truth, the golden rope glowing with magical energy.\"\n",
        "            ],\n",
        "            \"dialogues\": [\n",
        "                \"I am Diana of Themyscira, daughter of Hippolyta.\",\n",
        "                \"I fight for those who cannot fight for themselves.\",\n",
        "                \"Peace is a virtue to be shown, not peace enforced.\",\n",
        "                \"The truth will set you free.\"\n",
        "            ],\n",
        "            \"positions\": [(250, 70), (270, 60), (230, 100), (260, 80)],\n",
        "            \"story\": \"\"\"The sun rises over Themyscira, casting a golden glow on the island paradise. Diana stands proudly on a cliff overlooking the ocean, her armor gleaming in the morning light, ready to face whatever challenges the day may bring.\n",
        "\n",
        "News reaches the Amazon princess of danger in the world of men. Without hesitation, Diana leaves her homeland behind. In the midst of chaos, Wonder Woman stands firm, her bracelets deflecting a barrage of bullets meant to harm innocent bystanders.\n",
        "\n",
        "Ancient mythological creatures, released by a power-hungry sorcerer, terrorize a small village. Wonder Woman meets them in battle, her sword and shield moving with incredible speed and precision. Her training as an Amazon warrior guides each strike.\n",
        "\n",
        "With the creatures defeated, Wonder Woman confronts the sorcerer. She wraps her golden Lasso of Truth around him, the mystical rope glowing with divine energy. As the magic of the lasso compels him to speak honestly, the truth behind his actions is revealed, and justice can finally be served.\"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates.get(template_name.lower(), None)\n",
        "\n",
        "def get_available_templates():\n",
        "    return [\"Batman\", \"Spiderman\", \"WonderWoman\"]\n",
        "\n",
        "def generate_template_comic(template_name, num_panels=4):\n",
        "    template_data = get_template_data(template_name.lower())\n",
        "\n",
        "    if not template_data:\n",
        "        print(f\"Template '{template_name}' not found. Available templates: {', '.join(get_available_templates())}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    try:\n",
        "        pipe = load_sdxl_model()\n",
        "\n",
        "        print(f\"\\nGenerating {template_name} comic panels...\")\n",
        "        panel_images = []\n",
        "\n",
        "        print(\"Generating base character image...\")\n",
        "        base_prompt = f\"A detailed illustration of {template_name} in a dynamic pose, full body visible\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        prompts = template_data[\"prompts\"][:num_panels]\n",
        "        dialogues = template_data[\"dialogues\"][:num_panels]\n",
        "        positions = template_data[\"positions\"][:num_panels]\n",
        "\n",
        "        while len(prompts) < num_panels:\n",
        "            prompts.append(f\"{template_name} in an action scene\")\n",
        "        while len(dialogues) < num_panels:\n",
        "            dialogues.append(\"\")\n",
        "        while len(positions) < num_panels:\n",
        "            positions.append((256, 100))\n",
        "\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.65,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                image = add_speech_bubble(image, dialogues[i], positions[i])\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(f\"{template_name}_comic_strip.png\")\n",
        "        print(f\"{template_name} comic generation complete!\")\n",
        "\n",
        "        # Use the pre-written story from the template data\n",
        "        story_text = template_data[\"story\"]\n",
        "        story_panels = prompts\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating template comic: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_comic(story_prompt, num_panels=4, model_path=None, art_style=\"cartoon style\", use_template=None):\n",
        "    if use_template and use_template.lower() != \"none\":\n",
        "        return generate_template_comic(use_template, num_panels)\n",
        "\n",
        "    if len(story_prompt) < 10:\n",
        "        story_prompt = f\"Create an interesting story about {story_prompt}\"\n",
        "\n",
        "    try:\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        model, tokenizer = load_fine_tuned_model(model_path)\n",
        "\n",
        "        if model is None or tokenizer is None:\n",
        "            raise Exception(\"Failed to load the fine-tuned model\")\n",
        "\n",
        "        print(f\"Generating story from prompt: '{story_prompt}'\")\n",
        "        story_text = generate_story(model, tokenizer, story_prompt)\n",
        "        print(\"Story generated:\")\n",
        "        print(story_text)\n",
        "\n",
        "        # If story generation still produces insufficient content after multiple attempts\n",
        "        # and with different parameters, try generating with a completely different approach\n",
        "        if not story_text or len(story_text) < 50:\n",
        "            print(\"Story generation produced insufficient content. Using alternative generation approach.\")\n",
        "            story_text = generate_alternative_story(story_prompt)\n",
        "            print(\"Alternative story generated:\")\n",
        "            print(story_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model-based story generation: {e}\")\n",
        "        print(\"Using alternative story generation approach...\")\n",
        "        story_text = generate_alternative_story(story_prompt)\n",
        "\n",
        "    print(\"\\nSplitting story into panels...\")\n",
        "    story_panels, panel_texts, dialogues = split_story_into_panels(story_text, num_panels)\n",
        "    for i, panel in enumerate(story_panels):\n",
        "        print(f\"Panel {i+1}: {panel}\")\n",
        "        if dialogues[i]:\n",
        "            print(f\"Dialogue for panel {i+1}: \\\"{dialogues[i]}\\\"\")\n",
        "\n",
        "    try:\n",
        "        pipe = load_custom_model()\n",
        "\n",
        "        print(\"\\nGenerating panel images...\")\n",
        "        panel_images = []\n",
        "\n",
        "        character_name = extract_character_name(story_prompt) or \"the character\"\n",
        "\n",
        "        print(\"Generating base character image for consistency...\")\n",
        "        base_prompt = f\"A friendly {art_style} illustration of {character_name}, whole body visible, simple background, bright colors, child-friendly\"\n",
        "\n",
        "        is_sdxl = isinstance(pipe, StableDiffusionXLPipeline)\n",
        "\n",
        "        if is_sdxl:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30\n",
        "            ).images[0]\n",
        "        else:\n",
        "            base_image = pipe(\n",
        "                prompt=base_prompt,\n",
        "                negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "        base_image = base_image.resize((768, 768))\n",
        "\n",
        "        for i, panel_text in enumerate(panel_texts):\n",
        "            print(f\"Generating image for panel {i+1}...\")\n",
        "            # Make the image prompts unique for each panel based on the actual story content\n",
        "            prompt = f\"A friendly {art_style} illustration for a children's storybook showing: {panel_text}, featuring {character_name}, bright colors, simple shapes, cute characters, child-friendly\"\n",
        "\n",
        "            if is_sdxl:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipe(\n",
        "                    prompt=prompt,\n",
        "                    image=base_image,\n",
        "                    strength=0.7,\n",
        "                    negative_prompt=\"blurry, low quality, text, watermark, signature, bad proportions, scary, dark, adult themes\",\n",
        "                    num_inference_steps=30,\n",
        "                    guidance_scale=7.5\n",
        "                ).images[0]\n",
        "\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            if dialogues[i]:\n",
        "                position = (256, 100 + (i % 2) * 50)\n",
        "                image = add_speech_bubble(image, dialogues[i], position)\n",
        "\n",
        "            panel_images.append(image)\n",
        "\n",
        "            image_path = f\"panel_{i+1}.png\"\n",
        "            image.save(image_path)\n",
        "\n",
        "        print(\"Creating combined comic strip...\")\n",
        "        rows = int(np.ceil(num_panels / 2))\n",
        "        columns = min(2, num_panels)\n",
        "\n",
        "        final_width = 512 * columns\n",
        "        final_height = 512 * rows\n",
        "\n",
        "        comic_strip = Image.new(\"RGB\", (final_width, final_height), (255, 255, 255))\n",
        "\n",
        "        for i, image in enumerate(panel_images):\n",
        "            x_offset = (i % 2) * 512\n",
        "            y_offset = (i // 2) * 512\n",
        "            comic_strip.paste(image, (x_offset, y_offset))\n",
        "\n",
        "        comic_strip.save(\"comic_strip.png\")\n",
        "        print(\"Comic generation complete! All panels saved as PNG files and combined into comic_strip.png\")\n",
        "\n",
        "        return story_text, story_panels, panel_images, comic_strip\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in image generation: {e}\")\n",
        "        print(\"Unable to generate images. Please check if you have enough GPU memory and proper installations.\")\n",
        "        return story_text, story_panels, None, None\n",
        "\n",
        "# Web UI using Gradio\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Story to Comic Generator with InkoloRA\")\n",
        "        gr.Markdown(\"Enter a prompt to generate a story and convert it into comic panels with speech bubbles using the InkoloRA model\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                template_dropdown = gr.Dropdown(\n",
        "                    choices=[\"None\"] + get_available_templates(),\n",
        "                    label=\"Choose Template (optional)\",\n",
        "                    value=\"None\"\n",
        "                )\n",
        "\n",
        "                story_prompt = gr.Textbox(\n",
        "                    label=\"Story Prompt\",\n",
        "                    placeholder=\"Enter a prompt for the children's story (e.g. 'a little boy who learns to fly')\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                num_panels = gr.Slider(\n",
        "                    label=\"Number of Panels\",\n",
        "                    minimum=2,\n",
        "                    maximum=8,\n",
        "                    value=4,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "                art_style = gr.Dropdown(\n",
        "                    choices=[\"Cartoon Style\", \"Realistic\", \"Anime\", \"Sketch\"],\n",
        "                    label=\"Art Style\",\n",
        "                    value=\"Cartoon Style\"\n",
        "                )\n",
        "\n",
        "                generate_button = gr.Button(\"Generate Comic\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Story\",\n",
        "                    placeholder=\"The generated story will appear here...\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                output_panels = gr.Gallery(\n",
        "                    label=\"Comic Panels\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"comic-panels\",\n",
        "                    height=200,\n",
        "                    columns=2\n",
        "                )\n",
        "\n",
        "                output_comic = gr.Image(\n",
        "                    label=\"Combined Comic Strip\",\n",
        "                    type=\"pil\"\n",
        "                )\n",
        "\n",
        "        # Fixed function to handle the generation and properly return outputs\n",
        "        def generate_comic_action(story_prompt, num_panels, art_style, use_template):\n",
        "            story_text, story_panels, panel_images, comic_strip = generate_comic(\n",
        "                story_prompt, num_panels, art_style=art_style, use_template=use_template\n",
        "            )\n",
        "            # Return the outputs directly rather than updating them\n",
        "            return story_text, panel_images if panel_images else [], comic_strip\n",
        "\n",
        "        generate_button.click(\n",
        "            generate_comic_action,\n",
        "            inputs=[story_prompt, num_panels, art_style, template_dropdown],\n",
        "            outputs=[output_text, output_panels, output_comic]\n",
        "        )\n",
        "\n",
        "    app.launch()\n",
        "\n",
        "# Mount Google Drive if in Colab environment\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab or drive already mounted\")\n",
        "\n",
        "# Call the function to create the UI\n",
        "if __name__ == \"__main__\":  # Fixed from 'name' to 'name_'\n",
        "    create_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "0z1Mj74WPduI",
        "outputId": "ad0c67a9-3a8b-4669-e82b-ca7905699571"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://82163ff8ae0f07cbf1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://82163ff8ae0f07cbf1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M57re1dyQA9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}